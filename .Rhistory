na_attr %>%
mutate(percent = na_count / nrow(train_join)) %>%
arrange(desc(percent))
na_attr %>%
mutate(percent = na_count / nrow(train_join)) %>%
arrange(percent)
na_attr <- summarise_each(train_join, list(~sum(is.na(.))))
na_attr <- na_attr[, colSums(na_attr) > 0]
x <- as.vector(na_attr)
x
x <- unlist(na_attr)
na_attr <- tibble(col_name = colnames(na_attr), na_count = unname(unlist(na_attr)))
na_attr <- tibble(col_name = colnames(na_attr), na_count = unname(unlist(na_attr))) %>%
mutate(group = factor(unlist(map(na_count, function(x) which(unique(na_count) == x)))))
View(na_attr)
map(na_count, function(x) which(unique(na_count) == x))
map(na_attr$na_count, function(x) which(unique(na_attr$na_count) == x))
unlist(map(na_attr$na_count, function(x) which(unique(na_attr$na_count) == x)))
length(unlist(map(na_attr$na_count, function(x) which(unique(na_attr$na_count) == x))))
na_attr <- summarise_each(train_join, list(~sum(is.na(.))))
na_attr <- na_attr[, colSums(na_attr) > 0]
na_attr <- tibble(col_name = colnames(na_attr), na_count = unname(unlist(na_attr))) %>%
mutate(group = factor(unlist(map(na_count, function(x) which(unique(na_count) == x)))))
na_attr
na_attr <- tibble(col_name = colnames(na_attr), na_count = unname(unlist(na_attr))) %>%
mutate(group = factor(unlist(map(na_count, function(x) which(unique(na_count) == x)))),
percent = na_count / nrow(train_join))
na_attr <- summarise_each(train_join, list(~sum(is.na(.))))
na_attr <- na_attr[, colSums(na_attr) > 0]
na_attr <- tibble(col_name = colnames(na_attr), na_count = unname(unlist(na_attr))) %>%
mutate(group = factor(unlist(map(na_count, function(x) which(unique(na_count) == x)))),
percent = na_count / nrow(train_join))
View(na_attr)
##NA's to remove
rem_na <- filter(na_attr, percent > 0.9)
View(rem_na)
##NA's to remove
rem_na <- filter(na_attr, percent > 0.9) %>% select(col_name)
unlist(rem_na)
unname(unlist(rem_na))
##NA's to remove
rem_na <- filter(na_attr, percent > 0.8) %>% select(col_name)
##NA's to remove
rem_na <- filter(na_attr, percent > 0.9) %>% select(col_name) %>% unname(unlist(.))
filter(na_attr, percent > 0.9) %>% select(col_name) %>% unname(unlist(.))
filter(na_attr, percent > 0.9) %>% select(col_name) %>% unname(unlist())
##NA's to remove
rem_na <- unname(unlist(filter(na_attr, percent > 0.9) %>% select(col_name)))
train_join <- train_join[ , which(!colnames(train_join) %in% rem_na)]
t_train <- sample_frac(train_join, size = 0.3)
t_test <- setdiff(train_join, t_train)
gc()
table(apply(t_train, 1, function(r) all(!(is.na(r)))))
na_attr <- tibble(col_name = colnames(na_attr_train), na_count = unname(unlist(na_attr_train))) %>%
mutate(group = factor(unlist(map(na_count, function(x) which(unique(na_count) == x)))),
percent = na_count / nrow(t_train))
na_attr_train <- tibble(col_name = colnames(na_attr_train), na_count = unname(unlist(na_attr_train))) %>%
mutate(group = factor(unlist(map(na_count, function(x) which(unique(na_count) == x)))),
percent = na_count / nrow(t_train))
na_attr <- summarise_each(train_join, list(~sum(is.na(.))))
na_attr <- na_attr[, colSums(na_attr) > 0]
na_attr <- tibble(col_name = colnames(na_attr), na_count = unname(unlist(na_attr))) %>%
mutate(group = factor(unlist(map(na_count, function(x) which(unique(na_count) == x)))),
percent = na_count / nrow(train_join))
rem_na <- unname(unlist(filter(na_attr_train, percent > 0.8) %>% select(col_name)))
t_train <- t_train[ , which(!colnames(t_train) %in% rem_na)]
table(apply(t_train, 1, function(r) all(!(is.na(r)))))
t_train
na_attr_train <- summarise_each(t_train, list(~sum(is.na(.))))
na_attr_train <- na_attr_train[, colSums(na_attr_train) > 0]
t_train <- sample_frac(train_join, size = 0.3)
t_test <- setdiff(train_join, t_train)
t_train_preproc <- preProcess(t_train[ , names(na_attr)], method = 'bagImpute')
names(na_attr)
t_train_preproc <- preProcess(t_train[ , na_attr$col_name], method = 'bagImpute')
gc()
t_train_preproc <- preProcess(t_train[ , na_attr$col_name[which(!na_attr$col_name %in% cat_var)]], method = 'bagImpute')
gc()
t_train_preproc <- preProcess(t_train[ , na_attr$col_name[which(!na_attr$col_name %in% cat_var)]], method = 'bagImpute')
t_train[ , na_attr$col_name[which(!na_attr$col_name %in% cat_var)]]
ncol(t_train[ , na_attr$col_name[which(!na_attr$col_name %in% cat_var)]])
gc(reset = T)
memory.size()
memory.limit()
version
memory.limit(size = 8000)
memory.limit(size = 8126)
memory.size(max = TRUE)
memory.size()
4297.6/1024
gc(reset = T)
memory.limit(size = 8126)
memory.size()
memory.size(max = T)
object.size(train_join)
object.size(train_join)/1000000
rm(rem_na)
rm(x)
object.size(t_train_preproc)/1000000
rm(t_train_preproc)
rm(t_test)
rm(train_join)
t_train_preproc <- preProcess(t_train[ , na_attr$col_name[which(!na_attr$col_name %in% cat_var)]], method = 'bagImpute')
yandex_geocode <- function(search_line, apikey, ...) {
start_time <- Sys.time()
require(tidyverse)
require(xml2)
geo_find <- function(geocode, rspn = 0, bbox = NULL, coord_left_low, coord_right_up) {
#Combine a complete url for request
geocode <- paste(unlist(strsplit(geocode, split = " ")), collapse = "+")
if (rspn > 0 & is.null(bbox) == FALSE) {
coord1 <- unlist(strsplit(coord_left_low, split = ", "))
coord1 <- paste(coord1[2], coord1[1], sep = ",")
coord2 <- unlist(strsplit(coord_right_up, split = ", "))
coord2 <- paste(coord2[2], coord2[1], sep = ",")
url <- gsub(" ", "", paste('https://geocode-maps.yandex.ru/1.x?apikey=', apikey, "&geocode=", geocode, "&rspn=", rspn, "&bbox=", coord1, "~", coord2, collapse = ""))
} else {
url <- gsub(" ", "", paste('https://geocode-maps.yandex.ru/1.x?apikey=', apikey, "&geocode=", geocode, collapse = ""))
}
result <- as_list(read_xml(url))
result_to_parse <- result$ymaps$GeoObjectCollection$featureMember$GeoObject$metaDataProperty$GeocoderMetaData
found_add <- unlist(lapply(list(result_to_parse$text,
result[["ymaps"]][["GeoObjectCollection"]][["featureMember"]][["GeoObject"]][["Point"]][["pos"]][[1]],
result_to_parse$kind,
result_to_parse$precision,
result_to_parse$AddressDetails$Country$CountryName,
result_to_parse$AddressDetails$Country$AdministrativeArea$AdministrativeAreaName,
result_to_parse$AddressDetails$Country$AdministrativeArea$Locality$LocalityName,
result_to_parse$AddressDetails$Country$AdministrativeArea$Locality$Thoroughfare$ThoroughfareName,
result_to_parse$AddressDetails$Country$AdministrativeArea$Locality$Thoroughfare$Premise$PremiseNumber), function(x) ifelse(is.null(x) == T, NA, x)))
return(found_add)
}
geocode_result <- lapply(search_line, function(x) tryCatch(geo_find(x),
error = function(e) 'error'))
geocode_result <- as.data.frame(do.call(rbind, geocode_result))
colnames(geocode_result) <- c('AddressLine', 'point',	'kind', 'precision', 'Country', 'AdministrativeAreaName',	'LocalityName',	'ThoroughfareName',	'PremiseNumber')
end_time <- Sys.time()
return(geocode_result)
print(end_time - start_time)
}
library(openxlsx)
addr_belg <- read.xlsx('C:\\Users\\User\\Desktop\\Адреса Белгород.xlsx')
result <- yandex_geocode(addr_belg$Адрес, '2ed244eb-29c9-49fa-8508-80a84c1d69b0')
write.xlsx(cbind(addr_belg, result), '2TP_Belgorod.xlsx')
search_vm <- function(old_clust, new_clust, our_tbl, write_total = F){
require(jsonlite)
require(tidyverse)
require(openxlsx)
old_cluster <- fromJSON(old_clust)
old_cluster <- old_cluster$data
old_cluster$type <- 'old'
new_cluster <- fromJSON(new_clust)
new_cluster <- new_cluster$data
new_cluster$type <- 'new'
server_table <- read.xlsx(our_tbl)
server_table <- filter(server_table,is.na(Сервер)==F)
server_table$Сервер <- gsub("'", "", server_table$Сервер) %>% trimws()
server_table$name <- str_extract(server_table$Сервер, "\\(+[:print:]+\\)") %>% str_remove("\\(") %>% str_remove("\\)") %>% trimws(which = 'both')
server_table$vm_node <- sapply(server_table$Сервер, function(x) tail(unlist(strsplit(x, "\\s")), 1))
all_clusters <- bind_rows(new_cluster, old_cluster)
comparing_tbl <- left_join(server_table, all_clusters, by = 'name')
if(write_total == T){write.xlsx(comparing_tbl)} else{
}
result <- comparing_tbl[] %>%
filter(is.na(vmid) == T) %>%
select(c('Сервер', 'vm_node', 'type'))
print(result)
cat("In our table, but not in json files:", nrow(result))
all_clusters_rsoo <- all_clusters %>%
filter(pool %in% 'rsoo')
check_result <- anti_join(all_clusters_rsoo, server_table, by = 'name') %>%
select(c('name', 'vmid', 'node', 'status', 'type'))
print(check_result)
cat("VM in rsoo pool, but not in our table: ", nrow(check_result))
all_clusters_no_pool <- all_clusters %>%
filter(is.na(pool) == T)
main <- str_detect(all_clusters_no_pool$name, 'main')
second <- str_detect(all_clusters_no_pool$name, 'second')
rsoo <- str_detect(all_clusters_no_pool$name, 'rsoo')
main_check <- all_clusters_no_pool[(main == T & is.na(main) == F),]
second_check <- all_clusters_no_pool[(second == T & is.na(second) == F),]
rsoo_check <- all_clusters_no_pool[(rsoo == T & is.na(rsoo) == F),]
check_result_2 <- bind_rows(main_check, second_check, rsoo_check)
check_result_2 <- anti_join(check_result_2, server_table, by = 'name') %>%
select(c('name', 'vmid', 'node', 'status', 'type'))
print(check_result_2)
cat("Probably our VM, but not in rsoo pool: ", nrow(check_result_2))
}
search_vm('old_cluster.json', 'new_cluster.json', 'RSOO_Сервер.xlsx')
load("C:/Users/User/Desktop/R/My_R/Random_vec.RData")
x <- rand_vect(339, 21019)
table(x)
x <- rand_vect(339, 21019, sd = 5)
table(x)
x <- rand_vect(339, 21019, sd = 10)
table(x)
write.csv(data.frame(x), '1.csv')
x
library(openxlsx)
write.xlsx(data.frame(x), '1.xlsx')
##First transport##
years <- c(2020, 2024)
garbage_density_container <- 0.12434437252
garbage_sealing_coeff <- 3.2
truck_speed <- 14
truck_unload_time <- 10
container_load_time <- 1
truck_volume <- 12
working_time <- 10
container_volume <- 1.1
mean_filling_coef <- 0.8
##Second transport##
truck_2_speed <- 40
load_unload_time_2 <- 1
working_time_2 <- 12
truck_capacity <- NULL
truck_volume_2 <- 30
garbage_density_truck <- 0.415
transport_needs <- function(cls, direct, years) {
require(openxlsx)
require(tidyverse)
years <- sort(years)
##First transport##
cls <- list.files()[grep('clusters', list.files())]
cls <- read.csv(cls, sep = ";",  dec = ",", stringsAsFactors = F)
unchange_object_load_reload_time <- round((truck_unload_time / 60) +
(round(((truck_volume * garbage_sealing_coeff) /
(container_volume*mean_filling_coef)) * container_load_time) / 60), digits = 2)
unchange_yearly_truck_capacity <- (truck_volume * garbage_sealing_coeff * garbage_density_container * 365) / 1000
cls_comp <- cls %>%
filter(Год %in% years) %>%
group_by(Год, Название.района, Наименование.ОИ) %>%
summarise(mass = sum(Масса.образованных.отходов..тыс..тонн),
distance = weighted.mean(Пробег.первого.звена..км, Масса.образованных.отходов..тыс..тонн)) %>%
mutate(trip_duartion = (distance / truck_speed) + unchange_object_load_reload_time,
trips_per_day = if_else(floor(working_time / trip_duartion) < 1, 1, floor(working_time / trip_duartion)),
mass_transported = trips_per_day * unchange_yearly_truck_capacity,
transport_needs = ceiling(mass / mass_transported))
cls_final <- cls_comp %>%
group_by(Название.района, Год) %>%
summarise(mass = sum(mass),
transport = sum(transport_needs))
cls_final_region <- unique(cls_final[ , 1])
cls_final_tm <- split(cls_final[ , -1], as.factor(cls_final$Год))
cls_final_tm <- do.call(cbind, cls_final_tm)
cls_final <- bind_cols(cls_final_region, cls_final_tm)
rem <- grep("Год", colnames(cls_final))
cls_final <- cls_final[ , -rem]
c_names <- colnames(cls_final)
c_names_mass <- grep("mass", c_names)
c_names_transport <- grep("transport", c_names)
c_names[c_names_mass] <- paste0("Масса ", years, "., тыс. т.")
c_names[c_names_transport] <- paste0("Необходимо мусоровозов ", years, "., шт.")
c_names[1] <- "Название района"
colnames(cls_final) <- c_names
##Second transport##
if (is.null(truck_capacity) == T) {
truck_capacity <- truck_volume_2 * garbage_density_truck
}
direct <- list.files()[grep('directions', list.files())]
direct <- read.csv(direct, sep = ";",  dec = ",", stringsAsFactors = F)
direct_comp <- direct %>%
filter(Год %in% years & Тип.ОИ %in% c('Сортировка', 'recast', 'Перегрузка')) %>%
group_by(Год, Зона.РО.ОИ, Тип.ОИ, Наименование.ОИ, Тип.принимающего.ОИ, Наименование.принимающего.ОИ) %>%
summarise(mass = sum(Масса.отходов..отправленных.на.принимающий.ОИ..тыс..тонн),
distance = sum(Расстояние.до.принимающего.ОИ..км)) %>%
filter(is.na(mass) == F) %>%
mutate(distance = if_else(distance < 0.01, 0.1, distance),
trip_duration = (distance * (2 / truck_2_speed)) + load_unload_time_2,
trips_per_day = if_else((floor(working_time_2 / trip_duration)) < 1, 1, floor(working_time_2 / trip_duration)),
trips_per_year = ceiling((mass * 1000) / truck_capacity),
transport_needs = ceiling((trips_per_year / 365) / trips_per_day))
direct_final <- select(direct_comp, c(1:7, 12))
colnames(direct_final) <- c("Год", "Зона РО", "Тип ОИ", "Наименование ОИ", "Тип принимающего ОИ", "Наименование принимающего ОИ", "Масса отходов, отправленных на принимающий ОИ", "Требуется машин")
write.xlsx(list("I" = cls_final, "II" = direct_final), "Потребность в транспорте.xlsx")
}
mean_filling_coef <- 0.9
garbage_sealing_coeff <- 3
transport_needs("clusters.148.csv", "directions.148.csv", years = years)
transport_needs(years = years)
dir <- read.csv("directions.148.csv")
colnames(dir)
dir <- read.csv("directions.148.csv", sep = ";",  dec = ",", stringsAsFactors = F)
colnames(dir)
View(dir)
dir <- read.csv("directions.148.csv", sep = ";",  dec = ",", stringsAsFactors = F, na.strings = 0)
View(dir)
transport_needs <- function(cls, direct, years, only_first = F) {
require(openxlsx)
require(tidyverse)
years <- sort(years)
##First transport##
cls <- list.files()[grep('clusters', list.files())]
cls <- read.csv(cls, sep = ";",  dec = ",", stringsAsFactors = F)
unchange_object_load_reload_time <- round((truck_unload_time / 60) +
(round(((truck_volume * garbage_sealing_coeff) /
(container_volume*mean_filling_coef)) * container_load_time) / 60), digits = 2)
unchange_yearly_truck_capacity <- (truck_volume * garbage_sealing_coeff * garbage_density_container * 365) / 1000
cls_comp <- cls %>%
filter(Год %in% years) %>%
group_by(Год, Название.района, Наименование.ОИ) %>%
summarise(mass = sum(Масса.образованных.отходов..тыс..тонн),
distance = weighted.mean(Пробег.первого.звена..км, Масса.образованных.отходов..тыс..тонн)) %>%
mutate(trip_duartion = (distance / truck_speed) + unchange_object_load_reload_time,
trips_per_day = if_else(floor(working_time / trip_duartion) < 1, 1, floor(working_time / trip_duartion)),
mass_transported = trips_per_day * unchange_yearly_truck_capacity,
transport_needs = ceiling(mass / mass_transported))
cls_final <- cls_comp %>%
group_by(Название.района, Год) %>%
summarise(mass = sum(mass),
transport = sum(transport_needs))
cls_final_region <- unique(cls_final[ , 1])
cls_final_tm <- split(cls_final[ , -1], as.factor(cls_final$Год))
cls_final_tm <- do.call(cbind, cls_final_tm)
cls_final <- bind_cols(cls_final_region, cls_final_tm)
rem <- grep("Год", colnames(cls_final))
cls_final <- cls_final[ , -rem]
c_names <- colnames(cls_final)
c_names_mass <- grep("mass", c_names)
c_names_transport <- grep("transport", c_names)
c_names[c_names_mass] <- paste0("Масса ", years, "., тыс. т.")
c_names[c_names_transport] <- paste0("Необходимо мусоровозов ", years, "., шт.")
c_names[1] <- "Название района"
colnames(cls_final) <- c_names
if (only_first == T) {
write.xlsx(list("I" = cls_final), "Потребность в транспорте.xlsx")
}
##Second transport##
if (is.null(truck_capacity) == T) {
truck_capacity <- truck_volume_2 * garbage_density_truck
}
direct <- list.files()[grep('directions', list.files())]
direct <- read.csv(direct, sep = ";",  dec = ",", stringsAsFactors = F)
direct_comp <- direct %>%
filter(Год %in% years & Тип.ОИ %in% c('Сортировка', 'recast', 'Перегрузка')) %>%
group_by(Год, Зона.РО.ОИ, Тип.ОИ, Наименование.ОИ, Тип.принимающего.ОИ, Наименование.принимающего.ОИ) %>%
summarise(mass = sum(Масса.отходов..отправленных.на.принимающий.ОИ..тыс..тонн),
distance = sum(Расстояние.до.принимающего.ОИ..км)) %>%
filter(is.na(mass) == F) %>%
mutate(distance = if_else(distance < 0.01, 0.1, distance),
trip_duration = (distance * (2 / truck_2_speed)) + load_unload_time_2,
trips_per_day = if_else((floor(working_time_2 / trip_duration)) < 1, 1, floor(working_time_2 / trip_duration)),
trips_per_year = ceiling((mass * 1000) / truck_capacity),
transport_needs = ceiling((trips_per_year / 365) / trips_per_day))
direct_final <- select(direct_comp, c(1:7, 12))
colnames(direct_final) <- c("Год", "Зона РО", "Тип ОИ", "Наименование ОИ", "Тип принимающего ОИ", "Наименование принимающего ОИ", "Масса отходов, отправленных на принимающий ОИ", "Требуется машин")
write.xlsx(list("I" = cls_final, "II" = direct_final), "Потребность в транспорте.xlsx")
}
years <- c(2019)
garbage_density_container <- 0.12434437252
garbage_sealing_coeff <- 3
truck_speed <- 20
truck_unload_time <- 10
container_load_time <- 1
truck_volume <- 12
working_time <- 10
container_volume <- 1.1
mean_filling_coef <- 0.9
##Second transport##
truck_2_speed <- 40
load_unload_time_2 <- 1
working_time_2 <- 12
truck_capacity <- NULL
truck_volume_2 <- 30
garbage_density_truck <- 0.415
transport_needs(years = years, only_first = T)
transport_needs <- function(cls, direct, years, only_first = F) {
require(openxlsx)
require(tidyverse)
years <- sort(years)
##First transport##
cls <- list.files()[grep('clusters', list.files())]
cls <- read.csv(cls, sep = ";",  dec = ",", stringsAsFactors = F)
unchange_object_load_reload_time <- round((truck_unload_time / 60) +
(round(((truck_volume * garbage_sealing_coeff) /
(container_volume*mean_filling_coef)) * container_load_time) / 60), digits = 2)
unchange_yearly_truck_capacity <- (truck_volume * garbage_sealing_coeff * garbage_density_container * 365) / 1000
cls_comp <- cls %>%
filter(Год %in% years) %>%
group_by(Год, Название.района, Наименование.ОИ) %>%
summarise(mass = sum(Масса.образованных.отходов..тыс..тонн),
distance = weighted.mean(Пробег.первого.звена..км, Масса.образованных.отходов..тыс..тонн)) %>%
mutate(trip_duartion = (distance / truck_speed) + unchange_object_load_reload_time,
trips_per_day = if_else(floor(working_time / trip_duartion) < 1, 1, floor(working_time / trip_duartion)),
mass_transported = trips_per_day * unchange_yearly_truck_capacity,
transport_needs = ceiling(mass / mass_transported))
cls_final <- cls_comp %>%
group_by(Название.района, Год) %>%
summarise(mass = sum(mass),
transport = sum(transport_needs))
cls_final_region <- unique(cls_final[ , 1])
cls_final_tm <- split(cls_final[ , -1], as.factor(cls_final$Год))
cls_final_tm <- do.call(cbind, cls_final_tm)
cls_final <- bind_cols(cls_final_region, cls_final_tm)
rem <- grep("Год", colnames(cls_final))
cls_final <- cls_final[ , -rem]
c_names <- colnames(cls_final)
c_names_mass <- grep("mass", c_names)
c_names_transport <- grep("transport", c_names)
c_names[c_names_mass] <- paste0("Масса ", years, "., тыс. т.")
c_names[c_names_transport] <- paste0("Необходимо мусоровозов ", years, "., шт.")
c_names[1] <- "Название района"
colnames(cls_final) <- c_names
if (only_first == T) {
write.xlsx(list("I" = cls_final), "Потребность в транспорте.xlsx")
} else {
##Second transport##
if (is.null(truck_capacity) == T) {
truck_capacity <- truck_volume_2 * garbage_density_truck
}
direct <- list.files()[grep('directions', list.files())]
direct <- read.csv(direct, sep = ";",  dec = ",", stringsAsFactors = F)
direct_comp <- direct %>%
filter(Год %in% years & Тип.ОИ %in% c('Сортировка', 'recast', 'Перегрузка')) %>%
group_by(Год, Зона.РО.ОИ, Тип.ОИ, Наименование.ОИ, Тип.принимающего.ОИ, Наименование.принимающего.ОИ) %>%
summarise(mass = sum(Масса.отходов..отправленных.на.принимающий.ОИ..тыс..тонн),
distance = sum(Расстояние.до.принимающего.ОИ..км)) %>%
filter(is.na(mass) == F) %>%
mutate(distance = if_else(distance < 0.01, 0.1, distance),
trip_duration = (distance * (2 / truck_2_speed)) + load_unload_time_2,
trips_per_day = if_else((floor(working_time_2 / trip_duration)) < 1, 1, floor(working_time_2 / trip_duration)),
trips_per_year = ceiling((mass * 1000) / truck_capacity),
transport_needs = ceiling((trips_per_year / 365) / trips_per_day))
direct_final <- select(direct_comp, c(1:7, 12))
colnames(direct_final) <- c("Год", "Зона РО", "Тип ОИ", "Наименование ОИ", "Тип принимающего ОИ", "Наименование принимающего ОИ", "Масса отходов, отправленных на принимающий ОИ", "Требуется машин")
write.xlsx(list("I" = cls_final, "II" = direct_final), "Потребность в транспорте.xlsx")
}
}
transport_needs(years = years, only_first = T)
transport_needs(years = years, only_first = T)
garbage_sealing_coeff <- 2
transport_needs(years = years, only_first = T)
container_volume <- 0.75
transport_needs(years = years, only_first = T)
garbage_density_container <- 0.12434437252
garbage_sealing_coeff <- 2
truck_speed <- 14
truck_unload_time <- 10
container_load_time <- 1
truck_volume <- 12
working_time <- 8
container_volume <- 0.75
mean_filling_coef <- 0.9
transport_needs(years = years, only_first = T)
container_volume <- 1.1
transport_needs(years = years, only_first = T)
garbage_sealing_coeff <- 3
truck_speed <- 14
transport_needs(years = years, only_first = T)
setwd("C:/Users/User/Desktop/git_rep/My_rep/Usefull-datatools/gosjkh.ru")
load('html_table_advanced.RData')
gosjkh <- function(houses) {
require(rvest)
require(openxlsx)
require(tidyverse)
#Читаем главную страницу, собираем список всех ссылок,
#ведущих на страницы отдельных городов региона, где содержится информация о находящися в них МКД
houses_site <- read_html(houses)
info_nod <- html_nodes(houses_site, '.shadow-effect-2')
info_text <- html_text(info_nod, trim = T)
##Вывод информации об общей площади и числе домов региона, отраженных в базе
print(info_text)
houses_nodes <- html_nodes(houses_site, '.list-unstyled a')
houses_attr <- combine(html_attrs(houses_nodes))
all_links <- paste('http://gosjkh.ru', houses_attr, sep = "")
#Последовательно проходим по каждой ссылке из получившегося списка
function_result <- lapply(all_links, function(x) {
page <- read_html(x)
info_text_2 <- page %>% html_nodes('.shadow-effect-2') %>% html_text(trim = T)
##Вывод информации об общей площади и числе домов конкретного города, отраженных в базе
print(info_text_2)
last_page_nod <- page %>% html_nodes("ul")
##В завсмости от того, сколько страниц занимает список домов в городе (одну или больше), сразу скачиваем таблицу,
##или создаем еще один список, где будут содержаться ссылки на все подстраницы
if (length(xml_children(last_page_nod[[length(last_page_nod)]])) < 1) {
city_node <- html_nodes(page, 'body > div.wrapper > div.container.content')
city_node <- xml_find_first(city_node, './/table')
##Здесь задействуется функция 'html_table' из файла 'html_table_advanced'
city_data <- html_table(city_node, header = T, col_classes = list(`1`='integer',
`2`='character',
`3`='character',
`4`='character',
`5`='character',
`6`='character',
`7`='character'))[[1]]
return(city_data)
} else {
last_page <- xml_attrs(xml_child(xml_child(last_page_nod[[length(last_page_nod)]], length(xml_children(last_page_nod[[length(last_page_nod)]]))), 1)) %>%
str_split('=') %>% unlist()
city <- paste(x, "?page=", sep = "")
pages_num <- seq(1,as.numeric(last_page[2]))
city_pages <- paste(city, pages_num, sep = "")
city_data <- lapply(city_pages, . %>%
read_html %>%
html_nodes('body > div.wrapper > div.container.content') %>%
xml_find_first('.//table') %>%
html_table(header = T, col_classes=list(`1`='integer',`2`='character',`3`='character',`4`='character',`5`='character',`6`='character',`7`='character')))
##Объединяем собранные из нескольких подстраниц списки в один датафрейм
city_data <- do.call(bind_rows, city_data)
return(city_data)
}
})
#Объединяем результат выполнения функции в один датафрейм
#и сохраняем его как глобальную переменную для последующего анализа + записываем в xlsx
out <<- as.data.frame(do.call(rbind,function_result))
write.xlsx(out,'Гос ЖКХ.xlsx')
}
gosjkh("http://gosjkh.ru/houses/bryanskaya-oblast")
setwd("C:/Users/User/Desktop/git_rep/My_rep/Combined_app")
rm(out)
shiny::runApp()
runApp(host = "0.0.0.0", port = 4321)
